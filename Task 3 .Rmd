---
title: "Text Mining-Task 3"
author: "Xiang Li"
date: "12/7/2021"
output: 
  pdf_document:
  latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE )
library(gutenbergr)
library(dplyr)
library(stringr)
library(tidyr)
library(tidytext)
library(textdata)
library(tnum)
library(ggplot2)
library(magrittr)
library(utils)
library(wordcloud)
library(reshape2)
library(gridExtra)
library(magrittr)
library(tidyverse)
library(sentimentr)

```

# Task 3

I use the tnum ingested to load my book `My Doggie and I` into test2 number space. The name is `BallantyneR/DoggieandI`. Here is the verification. 

```{r}
library(tnum)
tnum.authorize("mssp1.bu.edu")
tnum.setSpace("test2")
source("Book2TN-v6A-1.R")
```

```{r}

Book_Doggie <- gutenberg_download(21752)
#write.table(Book_Doggie,'DoggieandI_text.txt',row.names = F)
#adding <> mannual into txt file
#Book_Doggie_tnum <- read.table("DoggieandI_text.txt", header = T)
#tnBooksFromLines(Book_Doggie_tnum$text, "BallantyneR/DoggieandI")
#tnum.getDBPathList(taxonomy="subject", levels=1)
doggie_v5 <- tnum.query("BallantyneR/DoggieandI/heading# has text",max=500)

doggie_vdf5 <- tnum.objectsToDf(doggie_v5)
head(doggie_vdf5$subject)
```



# Using sentimentr package do sentiment analysis

According to https://www.r-bloggers.com/2020/04/sentiment-analysis-in-r-with-sentimentr-that-handles-negation-valence-shifters/, I have the sentiment flow of the book sentence by sentence.

```{r fig.width=10, fig.height=4, fig.cap="Sentiment flow analysis"}
doggie_w5 <- tnum.query("BallantyneR/DoggieandI/section# has text",max=500)

doggie_wdf5 <- tnum.objectsToDf(doggie_w5)
#head(doggie_wdf5)

doggie.1 <- doggie_wdf5 %>% 
  get_sentences() %>% 
 sentiment()

plot_sentences_sentiment <- doggie.1%>% 
ggplot() + geom_col(aes(x= element_id,y = sentiment))
plot_sentences_sentiment
```

Here is the plot of density of average Sentiment of first 500 sentense.

```{r fig.width=10, fig.height=4, fig.cap="Average Sentiment of the book"}
plot_ave_sentiment <- doggie_wdf5%>% 
  get_sentences() %>% 
  sentiment_by(by = NULL) %>% 
  ggplot() + geom_density(aes(ave_sentiment))
plot_ave_sentiment

```
This figure displays the density of average sentiment of the first 500 sentence in `Doggie and I`.

```{r fig.width=6, fig.height=4,fig.align = "center", fig.cap="visualization of an estimate of the net sentiment (positive - negative) for each sentiment lexicon"}
doggie.2 <- doggie_wdf5 %>% 
  get_sentences()

word_task3 <- doggie.2 %>% 
  unnest_tokens(word, string.value)%>%
  anti_join(stop_words)

affin_task3 <- word_task3 %>%
inner_join(get_sentiments("afinn")) %>% 
  group_by(index = element_id) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc_task3 <- bind_rows(
  word_task3 %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  word_task3 %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = element_id, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(affin_task3, 
          bing_and_nrc_task3) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```
Discussion: Similarly to the task2, instead of using integer division to define larger sections of text every 20 line in the book, I use sentence level in task 3. I have three distinct lexicons for calculating sentiment flows, which produce findings that differ in absolute terms but follow similar relative paths across the novel. In the novel, I detect comparable drops and peaks in  emotion at roughly the same locations, but the absolute numbers are much different. The `AFINN` lexicon has the highest absolute values and the highest positive values. `Bing et al` lexicon has lower absolute values and appears to mark longer chunks of continuous positive or negative text. The `NRC` findings are skewed upward in comparison to the other two, positively labeling the text, but identifies identical relative changes in the text. 

Here is the summary of affin lexicon
```{r}
summary(affin_task3) 
```
```{r}

Bing <- word_task3 %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al.") %>%
  count(method, index = element_id , sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
  summary(Bing)
p1 <- ggplot(Bing, aes(x = sentiment, y = ..density..)) + 
  geom_histogram(binwidth = 0.25, fill = "#bada55", colour = "grey60") + 
  geom_density(size = 0.75)+
  ggtitle("Density Plot of Sentiment score in Bing Lexicon ")
  

nrc <- word_task3 %>% 
    inner_join(get_sentiments("nrc")) %>%
    mutate(method = "NRC.") %>%
  count(method, index = element_id , sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

p2 <- ggplot(nrc, aes(x = sentiment, y = ..density..)) + 
  geom_histogram(binwidth = 0.25, fill = "#bada55", colour = "grey60") + 
  geom_density(size = 0.75)+
  ggtitle("Density Plot of Sentiment score in NRC Lexicon ")
  

grid.arrange(p1,p2,ncol=1)

```


```{r}
bing_word_counts_task3 <- word_task3 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


# Extra Credit
### character tagging
```{r}
qr1 <- tnum.query("BallantyneR/DoggieandI/section:# has * = REGEXP(\" Willis\")")
qr1_df <-  tnum.objectsToDf(qr1)
View(qr1_df)
tnum.tagByQuery("BallantyneR/DoggieandI/section:# has * = REGEXP(\" Willis\")",
adds = ("reference:Willis"))
# now the quary for the tag gives you the same reference

qr2 <- tnum.query("@reference:Willis")
qr2_df <- tnum.objectsToDf(qr2)
head(qr2_df)
# now the quary for the tag gives you the same reference

```

